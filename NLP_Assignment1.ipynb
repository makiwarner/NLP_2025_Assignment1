{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_JvSieZ5t3vEzAbDTdV3HTf0DAc0523d",
      "authorship_tag": "ABX9TyPEnGol20Z0YpsV4ovMd/2v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makiwarner/NLP_2025_Assignment1/blob/main/NLP_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from collections import defaultdict\n",
        "import random\n"
      ],
      "metadata": {
        "id": "04ZsqcsS2hhG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR6PrWtR0Ilv",
        "outputId": "ef8e1bd6-b8b5-486d-edf6-fea70f1a4e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path =  \"/content/drive/MyDrive/shakespeare.txt\"\n"
      ],
      "metadata": {
        "id": "eZG9AOah1iFp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Data Preparation**\n",
        "\n",
        "Below, I am opening the Shakespearean text file that I saved in my drive in read mode.\n",
        "\n",
        "Then, I combine all lines, covert all letters to lowercase, and remove the punctuation. I do this with the text.translate() function which maps all the punctuation characters to '' - None.\n",
        "\n",
        "From there, I split the text into tokens (words) based on the white spaces."
      ],
      "metadata": {
        "id": "yp7_zEKD3cCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_text(path):\n",
        "    with open(path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    text = ' '.join(lines)  # Combine all lines into a single string\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    tokens = text.split()\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "vBbHnI3w2fQP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The create_bigrams() function is quite straightfoward: we loop through all the tokens and form a pair of each word + the following word."
      ],
      "metadata": {
        "id": "72hA8FDZMBH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bigrams(tokens):\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]\n",
        "    return bigrams"
      ],
      "metadata": {
        "id": "0nn1gSj43Jp2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of the fill_bigram_to_next_token_counts() function is to give us an idea of the possibility of words (tokens) that follow each bigram. Thus, it is necessary that both the bigrams and tokens are taken as inputs.\n",
        "\n",
        "From there, I established the structure of the returned output (from_bigram_to_next_token_counts) to be a dictionary. (defaultdict will apply 0 if the next word is not present). The dictionary will take a bigram as a key and another dictionary of the next token and its count as the value.\n",
        "\n"
      ],
      "metadata": {
        "id": "EwmwHNPvM8vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_bigram_to_next_token_counts(bigrams, tokens):\n",
        "    from_bigram_to_next_token_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for i in range(len(bigrams)):\n",
        "        bigram = bigrams[i]\n",
        "        if i + 2 < len(tokens):  #ensure there's a next token after the bigram\n",
        "            next_token = tokens[i + 2]\n",
        "            from_bigram_to_next_token_counts[bigram][next_token] += 1\n",
        "\n",
        "    return from_bigram_to_next_token_counts"
      ],
      "metadata": {
        "id": "Zh1H_dvA3ZK0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Probability Distribution**\n",
        "\n",
        "Calculate probabilities for tokens that follow each bigram.\n",
        "\n",
        "This function uses the calculate_bigram_probabilities() function to transform the dictionary from next_possible_word : count, to contain the probability of the next word instead."
      ],
      "metadata": {
        "id": "6WoiJ0033hdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bigram_probabilities(from_bigram_to_next_token_counts):\n",
        "    from_bigram_to_next_token_probs = {}\n",
        "\n",
        "    for bigram, next_token_counts in from_bigram_to_next_token_counts.items():\n",
        "        total_count = sum(next_token_counts.values())\n",
        "        from_bigram_to_next_token_probs[bigram] = {\n",
        "            token: count / total_count for token, count in next_token_counts.items()\n",
        "        }\n",
        "\n",
        "    return from_bigram_to_next_token_probs"
      ],
      "metadata": {
        "id": "xy8AWUWt4WM4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Sampling Next Token**\n",
        "\n",
        "Sample the next token based on the probability distribution.\n",
        "\n",
        "The goal here is to predict the next word that follows a given bigram using a weighted random sampling method. It takes into account both the actually probability of the word appearing, and randomness. Interestingly, this is more reflective of the real world than deterministically choosing the word with the highest probability."
      ],
      "metadata": {
        "id": "6Elx6Wu88Ozi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_next_token(bigram, bigram_to_next_token_probs):\n",
        "    if bigram not in bigram_to_next_token_probs:\n",
        "        return None  #return None if bigram is not in the dictionary\n",
        "\n",
        "    next_tokens = list(bigram_to_next_token_probs[bigram].keys())\n",
        "    probabilities = list(bigram_to_next_token_probs[bigram].values())\n",
        "\n",
        "    #random.choices for weighted sampling:\n",
        "    sampled_token = random.choices(next_tokens, weights=probabilities, k=1)[0]\n",
        "    return sampled_token"
      ],
      "metadata": {
        "id": "DAyJdmWb80WM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Generating Text**\n",
        "\n",
        "Generate text starting with the given bigram and sampling the next token iteratively.\n",
        "\n",
        "We rely on the above sample_next_token() function to generate our text.\n",
        "\n",
        "We start by applying sample_next_token() to the bigram in order to generate the 3rd word, and we append these words to the generated_text array. We continually repeat this process until we go through all bigrams and generate the next word."
      ],
      "metadata": {
        "id": "ygB_j-k6AywT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_bigram(start_bigram, num_words, bigram_to_next_token_probs):\n",
        "    current_bigram = start_bigram\n",
        "    generated_text = [current_bigram[0], current_bigram[1]]  # Start with the initial bigram\n",
        "\n",
        "    for _ in range(num_words - 2):  # Already have two words from the bigram\n",
        "        next_token = sample_next_token(current_bigram, bigram_to_next_token_probs)\n",
        "        if not next_token:\n",
        "            break  # Stop if no next token can be sampled\n",
        "\n",
        "        generated_text.append(next_token)\n",
        "        # Update the bigram to the most recent pair of words\n",
        "        current_bigram = (current_bigram[1], next_token)\n",
        "\n",
        "    return ' '.join(generated_text)"
      ],
      "metadata": {
        "id": "zS3ikxoqA1PL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test cases** for each function above - to print the output and verify the desired output"
      ],
      "metadata": {
        "id": "Jr4UojwAQkph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/drive/MyDrive/shakespeare.txt\"\n",
        "\n",
        "    tokens = load_and_preprocess_text(file_path)\n",
        "\n",
        "    bigrams = create_bigrams(tokens)\n",
        "\n",
        "    bigram_to_next_token_counts = fill_bigram_to_next_token_counts(bigrams, tokens)\n",
        "\n",
        "    bigram_to_next_token_probs = calculate_bigram_probabilities(bigram_to_next_token_counts)\n",
        "\n",
        "    example_bigram = ('to', 'be')\n",
        "    sampled_token = sample_next_token(example_bigram, bigram_to_next_token_probs)\n",
        "    print(f\"Sampled next token for {example_bigram}: {sampled_token}\") #verification of output\n",
        "\n",
        "    start_bigram = ('to', 'be')\n",
        "    num_words = 50\n",
        "    generated_text = generate_text_from_bigram(start_bigram, num_words, bigram_to_next_token_probs)\n",
        "    print(f\"Generated text: {generated_text}\") #verification of output\n",
        "\n",
        "    # Verification of outputs:\n",
        "    example_bigram = ('to', 'be')\n",
        "    if example_bigram in bigram_to_next_token_probs:\n",
        "        print(f\"Next token counts for {example_bigram}: {bigram_to_next_token_counts[example_bigram]}\")\n",
        "        print(f\"Next token probabilities for {example_bigram}: {bigram_to_next_token_probs[example_bigram]}\")\n",
        "    else:\n",
        "        print(f\"The bigram {example_bigram} does not exist in the text.\")\n",
        "    if sampled_token:\n",
        "        print(f\"Sampled next token for {example_bigram}: {sampled_token}\")\n",
        "    else:\n",
        "        print(f\"The bigram {example_bigram} does not exist in the probabilities dictionary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqbWTQrt3QjF",
        "outputId": "2e43d7cb-c7ad-43d5-cb96-7af9afa80ec9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: to be crossed prison my heart which i by this thou dost common grow that thou consumst thy self art so possessed with murdrous hate that gainst thy self to breed another thee or if they sing tis with so dull a cheer that leaves look pale dreading the winters\n",
            "Next token counts for ('to', 'be'): defaultdict(<class 'int'>, {'new': 1, 'die': 1, 'gone': 1, 'those': 1, 'deaths': 1, 'won': 1, 'assailed': 1, 'remembered': 1, 'with': 1, 'your': 1, 'praised': 1, 'then': 1, 'diseased': 1, 'vile': 1, 'receives': 1, 'so': 1, 'sure': 1, 'crossed': 1, 'my': 1, 'if': 1, 'invited': 1, 'only': 1, 'a': 1, 'beloved': 1, 'to': 1})\n",
            "Next token probabilities for ('to', 'be'): {'new': 0.04, 'die': 0.04, 'gone': 0.04, 'those': 0.04, 'deaths': 0.04, 'won': 0.04, 'assailed': 0.04, 'remembered': 0.04, 'with': 0.04, 'your': 0.04, 'praised': 0.04, 'then': 0.04, 'diseased': 0.04, 'vile': 0.04, 'receives': 0.04, 'so': 0.04, 'sure': 0.04, 'crossed': 0.04, 'my': 0.04, 'if': 0.04, 'invited': 0.04, 'only': 0.04, 'a': 0.04, 'beloved': 0.04, 'to': 0.04}\n",
            "Sampled next token for ('to', 'be'): only\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5**\n",
        "\n",
        " Change the bigram specific functions to n grams, and compare the results to the bigram performance."
      ],
      "metadata": {
        "id": "76BdjFuFCEqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ngrams(tokens, n):\n",
        "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "    return ngrams\n",
        "\n",
        "def fill_ngram_to_next_token_counts(ngrams, tokens, n): #create a dictionary where keys are n-grams and values are dictionaries of following token counts.\n",
        "    ngram_to_next_token_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for i in range(len(ngrams)):\n",
        "        ngram = ngrams[i]\n",
        "        if i + n < len(tokens):  # Ensure there's a next token after the n-gram\n",
        "            next_token = tokens[i + n]\n",
        "            ngram_to_next_token_counts[ngram][next_token] += 1\n",
        "\n",
        "    return ngram_to_next_token_counts\n",
        "\n",
        "def calculate_ngram_probabilities(ngram_to_next_token_counts):\n",
        "    ngram_to_next_token_probs = {}\n",
        "\n",
        "    for ngram, next_token_counts in ngram_to_next_token_counts.items():\n",
        "        total_count = sum(next_token_counts.values())\n",
        "        ngram_to_next_token_probs[ngram] = {\n",
        "            token: count / total_count for token, count in next_token_counts.items()\n",
        "        }\n",
        "\n",
        "    return ngram_to_next_token_probs\n",
        "\n",
        "def sample_next_token(ngram, ngram_to_next_token_probs):\n",
        "    if ngram not in ngram_to_next_token_probs:\n",
        "        return None  # if the n-gram is not in the dictionary\n",
        "\n",
        "    next_tokens = list(ngram_to_next_token_probs[ngram].keys())\n",
        "    probabilities = list(ngram_to_next_token_probs[ngram].values())\n",
        "\n",
        "    sampled_token = random.choices(next_tokens, weights=probabilities, k=1)[0]\n",
        "    return sampled_token\n",
        "\n",
        "def generate_text_from_ngram(start_ngram, num_words, ngram_to_next_token_probs):\n",
        "    current_ngram = start_ngram\n",
        "    generated_text = list(current_ngram)  #start with the initial n-gram\n",
        "\n",
        "    for _ in range(num_words - len(start_ngram)):\n",
        "        next_token = sample_next_token(current_ngram, ngram_to_next_token_probs)\n",
        "        if not next_token:\n",
        "            break\n",
        "\n",
        "        generated_text.append(next_token)\n",
        "        # Update the n-gram to include the most recent tokens\n",
        "        current_ngram = tuple(generated_text[-len(current_ngram):])\n",
        "\n",
        "    return ' '.join(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d2-Mm4KCPwz",
        "outputId": "e316c934-f754-4430-ba17-9e0f38e40cb6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using 2-grams:\n",
            "Generated text: the sonnets by william shakespeare from fairest creatures we desire increase that thereby beautys rose might never die but if the while i think good thoughts whilst other write good words and in my thought whose love to you as he shows now my greatest grief thou best of dearest\n",
            "\n",
            "Using 3-grams:\n",
            "Generated text: the sonnets by william shakespeare from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a\n",
            "\n",
            "Using 4-grams:\n",
            "Generated text: the sonnets by william shakespeare from fairest creatures we desire increase that thereby beautys rose might never die but as the riper should by time decease his tender heir might bear his memory but thou contracted to thine own bright eyes feedst thy lights flame with selfsubstantial fuel making a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test Cases** for task 5"
      ],
      "metadata": {
        "id": "DZpSYQQ9R2xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/drive/MyDrive/shakespeare.txt\"\n",
        "\n",
        "    tokens = load_and_preprocess_text(file_path)\n",
        "\n",
        "    n_values = [2, 3, 4]  # Bigrams, Trigrams, Quadgrams\n",
        "\n",
        "    for n in n_values:\n",
        "        print(f\"\\nUsing {n}-grams:\")\n",
        "\n",
        "        ngrams = create_ngrams(tokens, n)\n",
        "\n",
        "        ngram_to_next_token_counts = fill_ngram_to_next_token_counts(ngrams, tokens, n)\n",
        "\n",
        "        ngram_to_next_token_probs = calculate_ngram_probabilities(ngram_to_next_token_counts)\n",
        "\n",
        "        start_ngram = tuple(tokens[:n])\n",
        "        num_words = 50\n",
        "        generated_text = generate_text_from_ngram(start_ngram, num_words, ngram_to_next_token_probs)\n",
        "\n",
        "        print(f\"Generated text: {generated_text}\")"
      ],
      "metadata": {
        "id": "RrHg3ZSIR2B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison between the bigram, trigram, and quadgram:**\n",
        "\n",
        "I was surprised to see that all 3 models predicted the same first 18 words! Especially given that we are using a random weighted sampling method, I would have thought the stochastic element would make the word predictions different.\n",
        "\n",
        "Once the words began to deviate, it is clear that the 4-gram model is best given the words are more related to eachother.  However, it is still not very logicial, as it starts by talking about rose, then an heir, then eyes with flames and fuel."
      ],
      "metadata": {
        "id": "thT7reUyH1Tq"
      }
    }
  ]
}